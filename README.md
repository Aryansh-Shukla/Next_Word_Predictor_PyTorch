# FAQ-to-Next-Token LSTM

## 📌 Project Summary
A compact PyTorch notebook that trains a **next-token prediction LSTM** on a small FAQ-style dataset.  
It tokenizes text, builds a vocabulary, constructs incremental training sequences, pads them, and trains a simple LSTM (Embedding → LSTM → Linear) to predict the next word given a prefix.  

This is an **educational proof-of-concept** for sequence modeling and small-scale language generation.

---

## 🚀 What It Does
- Builds a vocabulary from question + answer text.  
- Converts sentences into incremental prefix → next-token training sequences.  
- Pads sequences to uniform length and creates a `DataLoader`.  
- Trains an LSTM model to predict the next token.  
- Provides a simple greedy prediction function for iterative text continuation.  

---

## 📂 Repository Layout
- `Next_Word_Predictor.ipynb` — full end-to-end pipeline (preprocessing, dataset, model, training, inference).   
- `README.md` — this file.  

---

## 📊 Data
- Input is a **plain-text FAQ string** (about course fees, duration, syllabus, etc.).  
- The document is split into sentences with `split("\n")`.  
- Each sentence is tokenized with `nltk.word_tokenize`.  
- Sequences are generated by taking progressively longer prefixes of each sentence.  

---

## 🧠 Model & Design Choices
- **Embedding**: maps token indices to dense vectors.  
- **Single-layer LSTM**: models sequence context.  
- **Linear layer**: maps final hidden state to vocabulary logits.  
- **Loss**: Cross-Entropy.  
- **Decoding**: Greedy (argmax).  

➡️ Rationale: A **minimal, easy-to-train architecture** that shows the mechanics of next-token modeling.

---

## 🏋️ Training Details
- Prefix → next-token sequences are created from each sentence.  
- Left-padded to max length in dataset.  
- Trained with mini-batches via `DataLoader`.  
- Optimizer: **Adam**, Loss: **CrossEntropyLoss**.  
- Logs per-epoch loss.  
- Use the `prediction` helper to generate continuations.  

---

## 🔎 Expected Behavior
- Produces **plausible continuations** for in-domain FAQ text.  
- Many `<unk>` tokens appear for out-of-domain queries.  
- Educational, not production-grade.  

---

## ⚠️ Limitations
- Very **small dataset** → limited generalization.  
- **Word-level tokenization only** (no BPE/subwords).  
- **Simple architecture** (single-layer LSTM).  
- **Greedy decoding** → deterministic, sometimes bland.  
- ❌ No safety/content filtering — not suitable for deployment.  

---

## 📦 Dependencies
- Python 3.8+  
- PyTorch  
- pandas  
- numpy  
- nltk  

👉 Install via pip:  
```bash
pip install torch pandas numpy nltk
```
---

## 🔁 Reproducibility Tips

- Set random seeds for Python, NumPy, PyTorch.

- Save the vocabulary and model weights.

- GPU training may require flags for full determinism.
---
## 📜 License & Contact

- License: MIT 
- Issues/queries: open a GitHub Issue or contact the repo owner.
---
