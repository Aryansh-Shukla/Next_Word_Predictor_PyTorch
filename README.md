# FAQ-to-Next-Token LSTM

## ğŸ“Œ Project Summary
A compact PyTorch notebook that trains a **next-token prediction LSTM** on a small FAQ-style dataset.  
It tokenizes text, builds a vocabulary, constructs incremental training sequences, pads them, and trains a simple LSTM (Embedding â†’ LSTM â†’ Linear) to predict the next word given a prefix.  

This is an **educational proof-of-concept** for sequence modeling and small-scale language generation.

---

## ğŸš€ What It Does
- Builds a vocabulary from question + answer text.  
- Converts sentences into incremental prefix â†’ next-token training sequences.  
- Pads sequences to uniform length and creates a `DataLoader`.  
- Trains an LSTM model to predict the next token.  
- Provides a simple greedy prediction function for iterative text continuation.  

---

## ğŸ“‚ Repository Layout
- `Next_Word_Predictor.ipynb` â€” full end-to-end pipeline (preprocessing, dataset, model, training, inference).   
- `README.md` â€” this file.  

---

## ğŸ“Š Data
- Input is a **plain-text FAQ string** (about course fees, duration, syllabus, etc.).  
- The document is split into sentences with `split("\n")`.  
- Each sentence is tokenized with `nltk.word_tokenize`.  
- Sequences are generated by taking progressively longer prefixes of each sentence.  

---

## ğŸ§  Model & Design Choices
- **Embedding**: maps token indices to dense vectors.  
- **Single-layer LSTM**: models sequence context.  
- **Linear layer**: maps final hidden state to vocabulary logits.  
- **Loss**: Cross-Entropy.  
- **Decoding**: Greedy (argmax).  

â¡ï¸ Rationale: A **minimal, easy-to-train architecture** that shows the mechanics of next-token modeling.

---

## ğŸ‹ï¸ Training Details
- Prefix â†’ next-token sequences are created from each sentence.  
- Left-padded to max length in dataset.  
- Trained with mini-batches via `DataLoader`.  
- Optimizer: **Adam**, Loss: **CrossEntropyLoss**.  
- Logs per-epoch loss.  
- Use the `prediction` helper to generate continuations.  

---

## ğŸ” Expected Behavior
- Produces **plausible continuations** for in-domain FAQ text.  
- Many `<unk>` tokens appear for out-of-domain queries.  
- Educational, not production-grade.  

---

## âš ï¸ Limitations
- Very **small dataset** â†’ limited generalization.  
- **Word-level tokenization only** (no BPE/subwords).  
- **Simple architecture** (single-layer LSTM).  
- **Greedy decoding** â†’ deterministic, sometimes bland.  
- âŒ No safety/content filtering â€” not suitable for deployment.  

---

## ğŸ“¦ Dependencies
- Python 3.8+  
- PyTorch  
- pandas  
- numpy  
- nltk  

ğŸ‘‰ Install via pip:  
```bash
pip install torch pandas numpy nltk
```
---

## ğŸ” Reproducibility Tips

- Set random seeds for Python, NumPy, PyTorch.

- Save the vocabulary and model weights.

- GPU training may require flags for full determinism.
---
## ğŸ“œ License & Contact

- License: MIT 
- Issues/queries: open a GitHub Issue or contact the repo owner.
---
